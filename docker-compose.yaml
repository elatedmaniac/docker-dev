version: '3.8'
services:
  cloudflared:
    image: cloudflare/cloudflared:latest
    command: tunnel run --token ${CLOUDFLARE_TOKEN}
    container_name: cloudflared
    restart: unless-stopped
    networks:
      default:
        ipv4_address: 172.18.0.2
  drawio:
    image: jgraph/drawio:latest
    container_name: drawio_container
    environment:
      - PUBLIC_DNS=drawio.elatedmaniac.io
      - LETS_ENCRYPT_ENABLED=true
    restart: unless-stopped
    networks:
      default:
        ipv4_address: 172.18.0.3
  anaconda:
    build:
      context: .
      dockerfile: Jupyter/Dockerfile
    container_name: anaconda_container
    volumes:
      - ./Jupyter/certs:/home/jupyter/.jupyter/ssl
      - ./Jupyter/data:/workspace
      - ./Jupyter/config/jupyter_application_config.py/:/home/jupyter/.jupyter/jupyter_application_config.py
      - ./logs:/workspace/logs
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - JUPYTER_PASSWORD=${JUPYTER_PASSWORD}
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
    restart: unless-stopped
    networks:
      default:
        ipv4_address: 172.18.0.4
  neo4j:
    build:
      context: ./neo4j
      args:
        NEO4J_AUTH: ${NEO4J_AUTH}
    container_name: neo4j_container
    environment:
      - NEO4J_AUTH=${NEO4J_AUTH}
      - NEO4J_PLUGINS=${NEO4J_PLUGINS}
    volumes:
      - ./neo4j/logs:/logs
      - ./neo4j/import:/import
      - ./neo4j/plugins:/plugins
      - ./neo4j/certificates:/var/lib/neo4j/certificates
    restart: unless-stopped
    networks:
      default:
        ipv4_address: 172.18.0.5

  nginx:
    build:
      context: ./Nginx
    container_name: nginx_proxy
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx/certificates:/etc/nginx/certificates
    depends_on:
      - anaconda
      - drawio
    restart: unless-stopped
    networks:
      default:
        ipv4_address: 172.18.0.6
  streamlit:
    build:
      context: .
      dockerfile: streamlit/Dockerfile
    container_name: streamlit
    environment:
      - PHI3_AGENT_URL=http://phi3_agent:8000
      - CODEGEMMA_AGENT_URL=http://codegemma_agent:8000
    ports:
      - "8501:8501"
    volumes:
      - ./streamlit/app:/app
      - ./streamlit/app/.streamlit:/root/.streamlit 
    restart: unless-stopped
    depends_on:
      - phi3_agent
      - codegemma_agent
    networks:
      default:
        ipv4_address: 172.18.0.7
  postgres:
    image: postgres:13
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
      - postgres-db-volume:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "airflow"]
      interval: 5s
      retries: 5
    restart: unless-stopped
    networks:
      default:
        ipv4_address: 172.18.0.8
  phi3:
    build:
      context: .
      dockerfile: ollama/Dockerfile
      args:
      - MODEL=${MODEL:-phi3:3.8b} 
    container_name: phi3
    environment:
      - MODEL=${MODEL:-phi3:3.8b}
    volumes:
      - ./ollama/data:/root/.ollama  
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "http://localhost:11434/api/tags"]
      interval: 60s
      timeout: 10s
      retries: 3
    networks:
      default:
        ipv4_address: 172.18.0.9
  codegemma:
    build:
      context: .
      dockerfile: ollama/Dockerfile
      args:
      - MODEL=${MODEL:-codegemma:latest} 
    container_name: codegemma
    environment:
      - MODEL=${MODEL:-codegemma:latest}
    volumes:
      - ./ollama/data:/root/.ollama  
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "http://localhost:11434/api/tags"]
      interval: 60s
      timeout: 10s
      retries: 3
    networks:
      default:
        ipv4_address: 172.18.0.10
  embeddings:
    build:
      context: .
      dockerfile: ollama/Dockerfile
      args:
      - MODEL=${MODEL:-nomic-embed-text} 
    container_name: embeddings
    environment:
      - MODEL=${MODEL:-nomic-embed-text}
    volumes:
      - ./embeddings/data:/root/.ollama  
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "http://localhost:11434/api/tags"]
      interval: 60s
      timeout: 10s
      retries: 3
    networks:
      default:
        ipv4_address: 172.18.0.11
  phi3_agent:
    build:
      context: ./agents
      dockerfile: phi3/Dockerfile
    container_name: phi3_agent
    ports:
      - "8001:8000"
    volumes:
      - ./agents/phi3/data:/app/data
    environment:
      - OLLAMA_BASE_URL=http://phi3:11434
    depends_on:
      - phi3
    networks:
      default:
        ipv4_address: 172.18.0.12

  codegemma_agent:
    build:
      context: ./agents
      dockerfile: codegemma/Dockerfile
    container_name: codegemma_agent
    ports:
      - "8002:8000"
    volumes:
      - ./agents/codegemma/data:/app/data
    environment:
      - OLLAMA_BASE_URL=http://codegemma:11435
    depends_on:
      - codegemma
    networks:
      default:
        ipv4_address: 172.18.0.13
  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./performance-monitoring/prometheus/config/prometheus.yml:/etc/prometheus/prometheus.yml
    restart: unless-stopped
    networks:
      default:
        ipv4_address: 172.18.0.14

  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:7.17.9
    container_name: elasticsearch
    environment:
      - discovery.type=single-node
      - bootstrap.memory_lock=true
      - ES_JAVA_OPTS=-Xms1g -Xmx1g
      #- xpack.security.enabled=true
    ports:
      - "9200:9200"
    volumes:
      - esdata:/usr/share/elasticsearch/data
    ulimits:
      memlock:
        soft: -1
        hard: -1
    networks:
      default:
        ipv4_address: 172.18.0.15

  kibana:
    image: docker.elastic.co/kibana/kibana:7.17.9
    container_name: kibana
    ports:
      - "5601:5601"
    depends_on:
      - elasticsearch
    environment:
      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200
    networks:
      default:
        ipv4_address: 172.18.0.16

  filebeat:
    image: docker.elastic.co/beats/filebeat:7.17.9
    container_name: filebeat
    user: root
    volumes:
      - type: bind
        source: 'C:/ProgramData/Docker/containers'
        target: '/var/lib/docker/containers'
        read_only: true
      - type: bind
        source: './filebeat/filebeat.yml'
        target: '/usr/share/filebeat/filebeat.yml'
        read_only: true
    depends_on:
      - elasticsearch
    command: ["--strict.perms=false"]
    networks:
      default:
        ipv4_address: 172.18.0.17
  metricbeat:
    image: docker.elastic.co/beats/metricbeat:7.17.9
    container_name: metricbeat
    user: root
    volumes:
      - type: bind
        source: './performance-monitoring/metricbeat/metricbeat.yml'
        target: '/usr/share/metricbeat/metricbeat.yml'
        read_only: true
      - type: bind
        source: '//var/run/docker.sock'
        target: '/var/run/docker.sock'
      - type: bind
        source: 'C:/ProgramData/docker/containers'
        target: '/hostfs/var/lib/docker/containers'
        read_only: true
      - type: bind
        source: 'C:/'
        target: '/hostfs/c'
        read_only: true
    depends_on:
      - elasticsearch
    command: ["--strict.perms=false"]
    networks:
      default:
        ipv4_address: 172.18.0.20

volumes:
  esdata:
    driver: local
  postgres-db-volume:

networks:
  default:
    driver: bridge
    ipam:
      config:
        - subnet: 172.18.0.0/24
